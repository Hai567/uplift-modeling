{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35bf35f",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87439803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (420000, 100)\n",
      "Y shape: (420000,)\n",
      "T shape: (420000,)\n"
     ]
    }
   ],
   "source": [
    "from causalml.dataset import make_uplift_classification\n",
    "\n",
    "# Define the treatment names based on the 6 treatments mentioned in the paper\n",
    "treatment_names = ['control', 'treatment_1', 'treatment_2', 'treatment_3', 'treatment_4', 'treatment_5', 'treatment_6']\n",
    "\n",
    "delta_uplift_increase = {\n",
    "    'control': 0.0, # <--- Uplift của Control = 0\n",
    "    'treatment_1': 0.05, 'treatment_2': 0.1, 'treatment_3': 0.12,\n",
    "    'treatment_4': 0.17, 'treatment_5': 0.2, 'treatment_6': 0.2\n",
    "}\n",
    "\n",
    "delta_uplift_decrease = {\n",
    "    'control': 0.0, # <--- Uplift của Control = 0\n",
    "    'treatment_1': 0.01, 'treatment_2': 0.02, 'treatment_3': 0.03,\n",
    "    'treatment_4': 0.05, 'treatment_5': 0.06, 'treatment_6': 0.07\n",
    "}\n",
    "n_uplift_increase_mix_informative = [0, 1, 2, 3, 4, 5, 6]\n",
    "n_uplift_decrease_mix_informative = [0, 1, 1, 1, 1, 1, 1]\n",
    "    \n",
    "# Generate the data\n",
    "df, x_names = make_uplift_classification(\n",
    "    n_samples=10000 * 7, # Adjust total samples to account for control + 6 treatments\n",
    "    treatment_name=treatment_names,\n",
    "    n_classification_features=100,\n",
    "    n_classification_informative=20,\n",
    "    n_classification_redundant=10,\n",
    "    n_classification_repeated=10,\n",
    "    positive_class_proportion=0.2,\n",
    "    delta_uplift_increase_dict=delta_uplift_increase,\n",
    "    delta_uplift_decrease_dict=delta_uplift_decrease,\n",
    "    n_uplift_increase_mix_informative_dict={t: v for t, v in zip(treatment_names, n_uplift_increase_mix_informative)},\n",
    "    n_uplift_decrease_mix_informative_dict={t: v for t, v in zip(treatment_names, n_uplift_decrease_mix_informative)},\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "X = df[x_names].values\n",
    "\n",
    "Y = df['conversion'].values\n",
    "\n",
    "treatment_map = {\n",
    "    'control': 0,\n",
    "    'treatment_1': 1,\n",
    "    'treatment_2': 2,\n",
    "    'treatment_3': 3,\n",
    "    'treatment_4': 4,\n",
    "    'treatment_5': 5,\n",
    "    'treatment_6': 6\n",
    "}\n",
    "T = df['treatment_group_key'].map(treatment_map).values\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Y shape: {Y.shape}\")\n",
    "print(f\"T shape: {T.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12066f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treatment_group_key\n",
      "treatment_2    70000\n",
      "treatment_6    70000\n",
      "treatment_4    70000\n",
      "treatment_3    70000\n",
      "treatment_5    70000\n",
      "treatment_1    70000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['treatment_group_key'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd6f2c",
   "metadata": {},
   "source": [
    "# Objective function for *optuna*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f348416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.end_to_end_pipeline import Dragonnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklift.metrics import uplift_auc_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def objective_inner_cv(trial, X, y, t):\n",
    "    \"\"\"\n",
    "    Hàm mục tiêu cho Optuna. \n",
    "    Thực hiện Inner Cross-Validation để đánh giá bộ tham số.\n",
    "    \"\"\"\n",
    "    # Hidden size & Outcome size là multiplier trên input_dim [cite: 843-844]\n",
    "    hidden_ratio = trial.suggest_float(\"hidden_ratio\", 0.5, 2.0)\n",
    "    outcome_ratio = trial.suggest_float(\"outcome_ratio\", 0.5, 2.0)\n",
    "    \n",
    "    alpha = trial.suggest_float(\"alpha\", 0.5, 1.5)\n",
    "    beta = trial.suggest_float(\"beta\", 0.5, 1.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256]) # Tăng batch size cho ổn định\n",
    "    \n",
    "    # Tính toán kích thước layer\n",
    "    input_dim = X.shape[1]\n",
    "    shared_hidden = int(input_dim * hidden_ratio)\n",
    "    outcome_hidden = int(input_dim * outcome_ratio)\n",
    "\n",
    "    # === B. Inner Cross-Validation Loop (5 folds theo paper) ===\n",
    "    # Để tiết kiệm thời gian demo, tôi để n_splits=3. Bạn hãy đổi thành 5 để giống hệt paper.\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    auuc_scores = []\n",
    "    \n",
    "    # Stratify theo Treatment để đảm bảo mỗi fold đều có đủ Control/Treatment\n",
    "    for train_idx, val_idx in skf.split(X, t):\n",
    "        X_train_inner, X_val_inner = X[train_idx], X[val_idx]\n",
    "        y_train_inner, y_val_inner = y[train_idx], y[val_idx]\n",
    "        t_train_inner, t_val_inner = t[train_idx], t[val_idx]\n",
    "        \n",
    "        # Khởi tạo model\n",
    "        model = Dragonnet(\n",
    "            input_dim=input_dim,\n",
    "            shared_hidden=shared_hidden,\n",
    "            outcome_hidden=outcome_hidden,\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            epochs=30, # Paper limit [cite: 837]\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            loss_type='tarreg',\n",
    "            device=device,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # Train (có thể dùng 1 phần validation nhỏ để early stopping hoặc train full epoch)\n",
    "        # Ở inner loop này ta train full 30 epochs cho đơn giản và nhanh\n",
    "        model.fit(X_train_inner, y_train_inner, t_train_inner, valid_perc=None)\n",
    "        \n",
    "        # Predict trên tập Validation của Inner fold\n",
    "        y0_pred, y1_pred, t_pred, eps = model.predict(X_val_inner)\n",
    "        uplift_pred = (y1_pred - y0_pred).flatten()\n",
    "        \n",
    "        # Tính metric: Uplift AUC\n",
    "        score = uplift_auc_score(\n",
    "            y_true=y_val_inner.flatten(),\n",
    "            uplift=uplift_pred,\n",
    "            treatment=t_val_inner.flatten()\n",
    "        )\n",
    "        auuc_scores.append(score)\n",
    "        \n",
    "    # Trả về trung bình AUUC của các fold\n",
    "    return np.mean(auuc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7590c204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 12:14:14,391] A new study created in memory with name: no-name-b18b2f1f-6482-475b-b1d7-d74b6de1e1b8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== PROCESSING TREATMENT 1 ====================\n",
      "\n",
      "--- Outer Fold 1/5 ---\n",
      "  -> Running TPE Search (Inner CV)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-12-20 12:14:23,499] Trial 0 failed with parameters: {'hidden_ratio': 0.5848954436477455, 'outcome_ratio': 1.8267322330109301, 'alpha': 1.4950664096373054, 'beta': 0.8664309261505964, 'learning_rate': 0.0007825794776104228, 'batch_size': 64} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ducha\\miniconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\ducha\\AppData\\Local\\Temp\\ipykernel_6708\\1801700088.py\", line 35, in <lambda>\n",
      "    lambda trial: objective_inner_cv(trial, X_outer_train, y_outer_train, t_outer_train),\n",
      "                  ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ducha\\AppData\\Local\\Temp\\ipykernel_6708\\2687006379.py\", line 59, in objective_inner_cv\n",
      "    model.fit(X_train_inner, y_train_inner, t_train_inner, valid_perc=None)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\projects\\dragonnet_research\\model\\end_to_end_pipeline.py\", line 148, in fit\n",
      "    loss.backward()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\ducha\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py\", line 625, in backward\n",
      "    torch.autograd.backward(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self, gradient, retain_graph, create_graph, inputs=inputs\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\ducha\\miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 354, in backward\n",
      "    _engine_run_backward(\n",
      "    ~~~~~~~~~~~~~~~~~~~~^\n",
      "        tensors,\n",
      "        ^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        accumulate_grad=True,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\ducha\\miniconda3\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 841, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        t_outputs, *args, **kwargs\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )  # Calls into the C++ engine to run the backward pass\n",
      "    ^\n",
      "KeyboardInterrupt\n",
      "[W 2025-12-20 12:14:23,508] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  -> Running TPE Search (Inner CV)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective_inner_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_outer_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_outer_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_outer_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Tắt progress bar con để đỡ rối\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m best_params = study.best_params\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  -> Best Params (Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ducha\\miniconda3\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ducha\\miniconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ducha\\miniconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ducha\\miniconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ducha\\miniconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  -> Running TPE Search (Inner CV)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m study.optimize(\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective_inner_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_outer_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_outer_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_outer_train\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[32m     36\u001b[39m     n_trials=\u001b[32m20\u001b[39m, \n\u001b[32m     37\u001b[39m     show_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# Tắt progress bar con để đỡ rối\u001b[39;00m\n\u001b[32m     38\u001b[39m )\n\u001b[32m     40\u001b[39m best_params = study.best_params\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  -> Best Params (Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mobjective_inner_cv\u001b[39m\u001b[34m(trial, X, y, t)\u001b[39m\n\u001b[32m     43\u001b[39m model = Dragonnet(\n\u001b[32m     44\u001b[39m     input_dim=input_dim,\n\u001b[32m     45\u001b[39m     shared_hidden=shared_hidden,\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m     seed=\u001b[32m42\u001b[39m\n\u001b[32m     55\u001b[39m )\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Train (có thể dùng 1 phần validation nhỏ để early stopping hoặc train full epoch)\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Ở inner loop này ta train full 30 epochs cho đơn giản và nhanh\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_train_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_perc\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Predict trên tập Validation của Inner fold\u001b[39;00m\n\u001b[32m     62\u001b[39m y0_pred, y1_pred, t_pred, eps = model.predict(X_val_inner)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\dragonnet_research\\model\\end_to_end_pipeline.py:148\u001b[39m, in \u001b[36mDragonnet.fit\u001b[39m\u001b[34m(self, X, y, T, valid_perc)\u001b[39m\n\u001b[32m    145\u001b[39m y0_pred, y1_pred, t_pred, eps = \u001b[38;5;28mself\u001b[39m.model(X)\n\u001b[32m    146\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.loss_f(y1, tr, t_pred, y0_pred, y1_pred, eps)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28mself\u001b[39m.optim.step()\n\u001b[32m    150\u001b[39m running_loss_train += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ducha\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ducha\\miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ducha\\miniconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "target_treatments = [1, 2, 3, 4, 5, 6] \n",
    "\n",
    "for treat_id in target_treatments:\n",
    "    print(f\"\\n{'='*20} PROCESSING TREATMENT {treat_id} {'='*20}\")\n",
    "    \n",
    "    # --- A. Lọc dữ liệu ---\n",
    "    mask = np.isin(T, [0, treat_id])\n",
    "    X_sub = X[mask]\n",
    "    Y_sub = Y[mask]\n",
    "    T_sub = T[mask]\n",
    "    \n",
    "    # Map Treatment ID về 1\n",
    "    T_sub_binary = np.where(T_sub == treat_id, 1, 0)\n",
    "    \n",
    "    # --- B. Outer Cross-Validation ---\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Danh sách để lưu kết quả và params\n",
    "    outer_results = [] \n",
    "    best_params_history = [] # <--- NEW: List lưu best params của từng fold\n",
    "    \n",
    "    fold_idx = 0\n",
    "    for train_idx, test_idx in outer_cv.split(X_sub, T_sub_binary):\n",
    "        fold_idx += 1\n",
    "        print(f\"\\n--- Outer Fold {fold_idx}/5 ---\")\n",
    "        \n",
    "        X_outer_train, X_outer_test = X_sub[train_idx], X_sub[test_idx]\n",
    "        y_outer_train, y_outer_test = Y_sub[train_idx], Y_sub[test_idx]\n",
    "        t_outer_train, t_outer_test = T_sub_binary[train_idx], T_sub_binary[test_idx]\n",
    "        \n",
    "        # --- C. Hyperparameter Tuning (TPE) ---\n",
    "        print(\"  -> Running TPE Search (Inner CV)...\")\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(\n",
    "            lambda trial: objective_inner_cv(trial, X_outer_train, y_outer_train, t_outer_train), \n",
    "            n_trials=20, \n",
    "            show_progress_bar=False # Tắt progress bar con để đỡ rối\n",
    "        )\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        print(f\"  -> Best Params (Fold {fold_idx}): {best_params}\")\n",
    "        \n",
    "        # --- D. Refit Final Model ---\n",
    "        print(\"  -> Refitting Final Model...\")\n",
    "        input_dim = X_outer_train.shape[1]\n",
    "        final_model = Dragonnet(\n",
    "            input_dim=input_dim,\n",
    "            shared_hidden=int(input_dim * best_params['hidden_ratio']),\n",
    "            outcome_hidden=int(input_dim * best_params['outcome_ratio']),\n",
    "            alpha=best_params['alpha'],\n",
    "            beta=best_params['beta'],\n",
    "            epochs=30,\n",
    "            batch_size=best_params['batch_size'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            loss_type='tarreg',\n",
    "            device=device,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        final_model.fit(X_outer_train, y_outer_train, t_outer_train, valid_perc=None)\n",
    "        \n",
    "        # --- E. Evaluate ---\n",
    "        y0_test_pred, y1_test_pred, _, _ = final_model.predict(X_outer_test)\n",
    "        uplift_test_pred = (y1_test_pred - y0_test_pred).flatten()\n",
    "        \n",
    "        final_auuc = uplift_auc_score(\n",
    "            y_true=y_outer_test.flatten(),\n",
    "            uplift=uplift_test_pred,\n",
    "            treatment=t_outer_test.flatten()\n",
    "        )\n",
    "        \n",
    "        outer_results.append(final_auuc)\n",
    "        print(f\"  -> Fold {fold_idx} Test AUUC: {final_auuc:.4f}\")\n",
    "        \n",
    "        # --- LƯU LẠI PARAMS VÀ KẾT QUẢ ---\n",
    "        best_params_history.append({\n",
    "            'fold': fold_idx,\n",
    "            'auuc': final_auuc,\n",
    "            'params': best_params\n",
    "        })\n",
    "\n",
    "    # --- KẾT QUẢ TỔNG HỢP CHO TREATMENT ---\n",
    "    mean_auuc = np.mean(outer_results)\n",
    "    std_auuc = np.std(outer_results)\n",
    "    \n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"SUMMARY FOR TREATMENT {treat_id}\")\n",
    "    print(f\"Mean AUUC = {mean_auuc:.4f} ± {std_auuc:.4f}\")\n",
    "    \n",
    "    # Tìm bộ tham số có kết quả Test cao nhất (Best of the Best)\n",
    "    best_run = max(best_params_history, key=lambda x: x['auuc'])\n",
    "    \n",
    "    print(\"\\n--- Best Parameters found in each Fold ---\")\n",
    "    for item in best_params_history:\n",
    "        print(f\"Fold {item['fold']} (AUUC {item['auuc']:.4f}): {item['params']}\")\n",
    "        \n",
    "    print(\"\\n--- SUGGESTED BEST PARAMS (From highest scoring fold) ---\")\n",
    "    print(f\"From Fold {best_run['fold']} with AUUC {best_run['auuc']:.4f}:\")\n",
    "    print(best_run['params'])\n",
    "    print(f\"{'#'*60}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
