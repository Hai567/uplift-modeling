{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185561c3",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-12-22T16:37:56.088477Z",
     "iopub.status.busy": "2025-12-22T16:37:56.087528Z",
     "iopub.status.idle": "2025-12-22T16:38:02.947368Z",
     "shell.execute_reply": "2025-12-22T16:38:02.946201Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 6.866576,
     "end_time": "2025-12-22T16:38:02.949488",
     "exception": false,
     "start_time": "2025-12-22T16:37:56.082912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\r\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\r\n",
      "Collecting scikit-uplift\r\n",
      "  Downloading scikit_uplift-0.5.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting causalml\r\n",
      "  Downloading causalml-0.15.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\r\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from scikit-uplift) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from scikit-uplift) (4.67.1)\r\n",
      "Collecting forestci==0.6 (from causalml)\r\n",
      "  Downloading forestci-0.6-py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Collecting pathos==0.2.9 (from causalml)\r\n",
      "  Downloading pathos-0.2.9-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from causalml) (0.14.5)\r\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (from causalml) (3.1.0)\r\n",
      "Requirement already satisfied: pydotplus in /usr/local/lib/python3.12/dist-packages (from causalml) (2.0.2)\r\n",
      "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (from causalml) (0.49.1)\r\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from causalml) (0.4.0)\r\n",
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (from causalml) (4.6.0)\r\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from causalml) (0.21)\r\n",
      "Requirement already satisfied: ppft>=1.7.6.5 in /usr/local/lib/python3.12/dist-packages (from pathos==0.2.9->causalml) (1.7.7)\r\n",
      "Requirement already satisfied: pox>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from pathos==0.2.9->causalml) (0.3.6)\r\n",
      "Requirement already satisfied: multiprocess>=0.70.13 in /usr/local/lib/python3.12/dist-packages (from pathos==0.2.9->causalml) (0.70.18)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->causalml) (1.0.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->scikit-uplift) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->scikit-uplift) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->scikit-uplift) (2.6.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->scikit-uplift) (2025.11.12)\r\n",
      "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap->causalml) (0.0.8)\r\n",
      "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap->causalml) (0.60.0)\r\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap->causalml) (3.1.1)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap->causalml) (0.43.0)\r\n",
      "Downloading scikit_uplift-0.5.1-py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading causalml-0.15.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading forestci-0.6-py3-none-any.whl (12 kB)\r\n",
      "Downloading pathos-0.2.9-py3-none-any.whl (76 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pathos, scikit-uplift, forestci, causalml\r\n",
      "  Attempting uninstall: pathos\r\n",
      "    Found existing installation: pathos 0.3.2\r\n",
      "    Uninstalling pathos-0.3.2:\r\n",
      "      Successfully uninstalled pathos-0.3.2\r\n",
      "Successfully installed causalml-0.15.5 forestci-0.6 pathos-0.2.9 scikit-uplift-0.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas torch scikit-learn matplotlib seaborn scikit-uplift causalml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe26841a",
   "metadata": {
    "papermill": {
     "duration": 0.003515,
     "end_time": "2025-12-22T16:38:02.956998",
     "exception": false,
     "start_time": "2025-12-22T16:38:02.953483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69c0c0bf",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-22T16:38:02.966231Z",
     "iopub.status.busy": "2025-12-22T16:38:02.965252Z",
     "iopub.status.idle": "2025-12-22T16:38:24.440128Z",
     "shell.execute_reply": "2025-12-22T16:38:24.439059Z"
    },
    "papermill": {
     "duration": 21.482044,
     "end_time": "2025-12-22T16:38:24.442419",
     "exception": false,
     "start_time": "2025-12-22T16:38:02.960375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\n",
      "  if entities is not ():\n",
      "Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "from causalml.dataset import make_uplift_classification\n",
    "\n",
    "# Define the treatment names based on the 6 treatments mentioned in the paper\n",
    "treatment_names = ['control', 'treatment_1', 'treatment_2', 'treatment_3', 'treatment_4', 'treatment_5', 'treatment_6']\n",
    "\n",
    "delta_uplift_increase = {\n",
    "    'control': 0.0, # <--- Uplift của Control = 0\n",
    "    'treatment_1': 0.05, 'treatment_2': 0.1, 'treatment_3': 0.12,\n",
    "    'treatment_4': 0.17, 'treatment_5': 0.2, 'treatment_6': 0.2\n",
    "}\n",
    "\n",
    "delta_uplift_decrease = {\n",
    "    'control': 0.0, # <--- Uplift của Control = 0\n",
    "    'treatment_1': 0.01, 'treatment_2': 0.02, 'treatment_3': 0.03,\n",
    "    'treatment_4': 0.05, 'treatment_5': 0.06, 'treatment_6': 0.07\n",
    "}\n",
    "n_uplift_increase_mix_informative = [0, 1, 2, 3, 4, 5, 6]\n",
    "n_uplift_decrease_mix_informative = [0, 1, 1, 1, 1, 1, 1]\n",
    "    \n",
    "# Generate the data\n",
    "df, x_names = make_uplift_classification(\n",
    "    n_samples=10000 * 7, # Adjust total samples to account for control + 6 treatments\n",
    "    treatment_name=treatment_names,\n",
    "    n_classification_features=100,\n",
    "    n_classification_informative=20,\n",
    "    n_classification_redundant=10,\n",
    "    n_classification_repeated=10,\n",
    "    positive_class_proportion=0.2,\n",
    "    delta_uplift_increase_dict=delta_uplift_increase,\n",
    "    delta_uplift_decrease_dict=delta_uplift_decrease,\n",
    "    n_uplift_increase_mix_informative_dict={t: v for t, v in zip(treatment_names, n_uplift_increase_mix_informative)},\n",
    "    n_uplift_decrease_mix_informative_dict={t: v for t, v in zip(treatment_names, n_uplift_decrease_mix_informative)},\n",
    "    random_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "725cd1be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T16:38:24.452097Z",
     "iopub.status.busy": "2025-12-22T16:38:24.450948Z",
     "iopub.status.idle": "2025-12-22T16:38:25.020490Z",
     "shell.execute_reply": "2025-12-22T16:38:25.019469Z"
    },
    "papermill": {
     "duration": 0.576075,
     "end_time": "2025-12-22T16:38:25.022233",
     "exception": false,
     "start_time": "2025-12-22T16:38:24.446158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (490000, 100)\n",
      "Y shape: (490000,)\n",
      "T shape: (490000,)\n"
     ]
    }
   ],
   "source": [
    "X = df[x_names].values\n",
    "\n",
    "Y = df['conversion'].values\n",
    "\n",
    "treatment_map = {\n",
    "    'control': 0,\n",
    "    'treatment_1': 1,\n",
    "    'treatment_2': 2,\n",
    "    'treatment_3': 3,\n",
    "    'treatment_4': 4,\n",
    "    'treatment_5': 5,\n",
    "    'treatment_6': 6\n",
    "}\n",
    "T = df['treatment_group_key'].map(treatment_map).values\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Y shape: {Y.shape}\")\n",
    "print(f\"T shape: {T.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da8ffc",
   "metadata": {
    "papermill": {
     "duration": 0.003698,
     "end_time": "2025-12-22T16:38:25.029720",
     "exception": false,
     "start_time": "2025-12-22T16:38:25.026022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# End to end pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb34064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T16:38:25.038846Z",
     "iopub.status.busy": "2025-12-22T16:38:25.038170Z",
     "iopub.status.idle": "2025-12-22T16:38:29.972388Z",
     "shell.execute_reply": "2025-12-22T16:38:29.971625Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 4.94147,
     "end_time": "2025-12-22T16:38:29.974538",
     "exception": false,
     "start_time": "2025-12-22T16:38:25.033068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Building model\n",
    "from functools import partial\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class DragonNetBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim: int\n",
    "        input dimension for convariates\n",
    "    shared_hidden: int\n",
    "        layer size for hidden shared representation layers\n",
    "    outcome_hidden: int\n",
    "        layer size for conditional outcome layers\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, shared_hidden=200, outcome_hidden=100):\n",
    "        super(DragonNetBase, self).__init__()\n",
    "        # NOTE: Shared representation layers - Dragon Body\n",
    "        self.full_connect_1 = nn.Linear(in_features=input_dim, out_features=shared_hidden)\n",
    "        self.full_connect_2 = nn.Linear(in_features=shared_hidden, out_features=shared_hidden)\n",
    "        self.full_connect_3 = nn.Linear(in_features=shared_hidden, out_features=shared_hidden)\n",
    "\n",
    "        # NOTE: Output of the Dragon Body\n",
    "        self.treat_out = nn.Linear(in_features=shared_hidden, out_features=1)\n",
    "        #---------------------------------------------------#\n",
    "\n",
    "        # NOTE: Prediction heads - 1st Dragon Head - Control\n",
    "        self.control_head_full_connect_1 = nn.Linear(in_features=shared_hidden, out_features=outcome_hidden)\n",
    "        self.control_head_full_connect_2 = nn.Linear(in_features=outcome_hidden, out_features=outcome_hidden)\n",
    "        self.control_head_full_connect_out = nn.Linear(in_features=outcome_hidden, out_features=1)\n",
    "\n",
    "        # NOTE: Prediction heads - 2nd Dragon Head - Treatment\n",
    "        self.treatment_head_full_connect_1 = nn.Linear(in_features=shared_hidden, out_features=outcome_hidden)\n",
    "        self.treatment_head_full_connect_2 = nn.Linear(in_features=outcome_hidden, out_features=outcome_hidden)\n",
    "        self.treatment_head_full_connect_out = nn.Linear(in_features=outcome_hidden, out_features=1)\n",
    "\n",
    "        # NOTE: Propensity score head - 3rd Dragon Head - uses linear epsilon\n",
    "        self.epsilon = nn.Linear(in_features=1, out_features=1)\n",
    "        torch.nn.init.xavier_normal_(self.epsilon.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        forward method to train model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: torch.Tensor\n",
    "            covariates\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y0: torch.Tensor\n",
    "            outcome under control\n",
    "        y1: torch.Tensor\n",
    "            outcome under treatment\n",
    "        t_pred: torch.Tensor\n",
    "            predicted treatment\n",
    "        eps: torch.Tensor\n",
    "            trainable epsilon parameter\n",
    "        \"\"\"\n",
    "        #shared layer\n",
    "        x = F.elu(self.full_connect_1(inputs))\n",
    "        x = F.elu(self.full_connect_2(x))\n",
    "        z = F.elu(self.full_connect_3(x))\n",
    "\n",
    "        #propensity\n",
    "        t_pred = torch.sigmoid(self.treat_out(z))\n",
    "\n",
    "        y0 = F.elu(self.control_head_full_connect_1(z))\n",
    "        y0 = F.elu(self.control_head_full_connect_2(y0))\n",
    "        y0 = self.control_head_full_connect_out(y0)\n",
    "\n",
    "        y1 = F.elu(self.treatment_head_full_connect_1(z))\n",
    "        y1 = F.elu(self.treatment_head_full_connect_2(y1))\n",
    "        y1 = self.treatment_head_full_connect_out(y1)\n",
    "\n",
    "        eps = self.epsilon(torch.ones_like(t_pred)[:, 0:1])\n",
    "\n",
    "        return y0, y1, t_pred, eps\n",
    "\n",
    "def default_loss(y_true, t_true, t_pred, y0_pred, y1_pred, eps, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Generic loss function for dragonnet\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: torch.Tensor\n",
    "        Actual target variable\n",
    "    t_true: torch.Tensor\n",
    "        Actual treatment variable\n",
    "    t_pred: torch.Tensor\n",
    "        Predicted treatment\n",
    "    y0_pred: torch.Tensor\n",
    "        Predicted target variable under control\n",
    "    y1_pred: torch.Tensor\n",
    "        Predicted target variable under treatment\n",
    "    eps: torch.Tensor\n",
    "        Trainable epsilon parameter\n",
    "    alpha: float\n",
    "        loss component weighting hyperparameter between 0 and 1\n",
    "    Returns\n",
    "    -------\n",
    "    loss: torch.Tensor\n",
    "    \"\"\"\n",
    "    t_pred = (t_pred + 0.01) / 1.02\n",
    "    loss_t = torch.sum(F.binary_cross_entropy(t_pred, t_true))\n",
    "\n",
    "    loss0 = torch.sum((1. - t_true) * torch.square(y_true - y0_pred))\n",
    "    loss1 = torch.sum(t_true * torch.square(y_true - y1_pred))\n",
    "\n",
    "    loss = loss0 + loss1 + alpha * loss_t\n",
    "\n",
    "    return loss\n",
    "\n",
    "def tarreg_loss(y_true, t_true, t_pred, y0_pred, y1_pred, eps, alpha=1.0, beta=1.0):\n",
    "    \"\"\"\n",
    "    Targeted regularisation loss function for dragonnet\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: torch.Tensor\n",
    "        Actual target variable\n",
    "    t_true: torch.Tensor\n",
    "        Actual treatment variable\n",
    "    t_pred: torch.Tensor\n",
    "        Predicted treatment\n",
    "    y0_pred: torch.Tensor\n",
    "        Predicted target variable under control\n",
    "    y1_pred: torch.Tensor\n",
    "        Predicted target variable under treatment\n",
    "    eps: torch.Tensor\n",
    "        Trainable epsilon parameter\n",
    "    alpha: float\n",
    "        loss component weighting hyperparameter between 0 and 1\n",
    "    beta: float\n",
    "        targeted regularization hyperparameter between 0 and 1\n",
    "    Returns\n",
    "    -------\n",
    "    loss: torch.Tensor\n",
    "    \"\"\"\n",
    "    vanilla_loss = default_loss(y_true, t_true, t_pred, y0_pred, y1_pred, alpha)\n",
    "    t_pred = (t_pred + 0.01) / 1.02\n",
    "\n",
    "    y_pred = t_true * y1_pred + (1 - t_true) * y0_pred\n",
    "\n",
    "    # clever covariate\n",
    "    h = (t_true / t_pred) - ((1 - t_true) / (1 - t_pred))\n",
    "\n",
    "    y_pert = y_pred + eps * h\n",
    "    targeted_regularization = torch.sum((y_true - y_pert)**2)\n",
    "\n",
    "    # final\n",
    "    loss = vanilla_loss + beta * targeted_regularization\n",
    "    return loss\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=15, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "class Dragonnet:\n",
    "    \"\"\"\n",
    "    Main class for the Dragonnet model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim: int\n",
    "        Input demension for convariates (X - features)\n",
    "    shared_hidden: int, default=200\n",
    "        The number of hidden layers in the dragon body\n",
    "    outcome_hidden: int, default=100\n",
    "        The number of hidden layers in the dragon accuracy head\n",
    "    alpha: float, default=1.0\n",
    "        loss component weighting hyperparameter between 0 and 1\n",
    "    beta: float, default=1.0\n",
    "        targeted regularization hyperparameter between 0 and 1\n",
    "    epochs: int, default=200\n",
    "        Number training epochs\n",
    "    batch_size: int, default=64\n",
    "        Training batch size\n",
    "    learning_rate: float, default=1e-3\n",
    "        Learning rate\n",
    "    data_loader_num_workers: int, default=4\n",
    "        Number of workers for data loader\n",
    "    loss_type: str, {'tarreg', 'default'}, default='tarreg'\n",
    "        Loss function to use\n",
    "    device=None\n",
    "        Whether we use the CPU or GPU to train\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim, # Input demension for convariates (X - features)\n",
    "            shared_hidden=200, # The number of hidden layers in the dragon body\n",
    "            outcome_hidden=100, # The number of hidden layers in the dragon accuracy head\n",
    "            alpha=1.0, #\n",
    "            beta=1.0,\n",
    "            epochs=30,\n",
    "            batch_size=32,\n",
    "            learning_rate=0.0005,\n",
    "            data_loader_num_workers=2,\n",
    "            loss_type=\"tarreg\",\n",
    "            device=None,\n",
    "            seed=42\n",
    "    ):\n",
    "        # 1. Thiết lập Device\n",
    "        if device:\n",
    "            self.model_device = device\n",
    "        else:\n",
    "            self.model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # 2. Khởi tạo Model và đưa lên Device ngay lập tức\n",
    "        self.model = DragonNetBase(input_dim=input_dim, shared_hidden=shared_hidden, outcome_hidden=outcome_hidden)\n",
    "        self.model.to(self.model_device) # Move model to GPU\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = data_loader_num_workers\n",
    "        self.seed = seed\n",
    "\n",
    "        # Optimizer phải được khởi tạo SAU KHI model đã được move lên GPU\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.train_dataloader = None\n",
    "        self.valid_dataloader = None\n",
    "\n",
    "        if loss_type == \"tarreg\":\n",
    "            self.loss_f = partial(tarreg_loss, alpha=alpha, beta=beta)\n",
    "        elif loss_type == \"default\":\n",
    "            self.loss_f = partial(default_loss, alpha=alpha)\n",
    "\n",
    "    def create_dataloaders(self, X, y, T, valid_perc=None):\n",
    "        \"\"\"\n",
    "        Utility function to create train and validation data loader:\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: np.array\n",
    "            covariates\n",
    "        y: np.array\n",
    "            target variable\n",
    "        T: np.array\n",
    "            treatment\n",
    "        \"\"\"\n",
    "        if valid_perc:\n",
    "            X_train, X_test, y_train, y_test, T_train, T_test = train_test_split(\n",
    "                X, y, T, test_size=valid_perc, random_state=self.seed\n",
    "            )\n",
    "            # Không cần .to(device) ở đây để tiết kiệm VRAM, sẽ move theo batch\n",
    "            X_train = torch.Tensor(X_train)\n",
    "            X_test = torch.Tensor(X_test)\n",
    "            y_train = torch.Tensor(y_train).reshape(-1, 1)\n",
    "            y_test = torch.Tensor(y_test).reshape(-1, 1)\n",
    "            T_train = torch.Tensor(T_train).reshape(-1, 1)\n",
    "            T_test = torch.Tensor(T_test).reshape(-1, 1)\n",
    "            train_dataset = TensorDataset(X_train, T_train, y_train)\n",
    "            valid_dataset = TensorDataset(X_test, T_test, y_test)\n",
    "            self.train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "            self.valid_dataloader = DataLoader(valid_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "        else:\n",
    "            X = torch.Tensor(X)\n",
    "            T = torch.Tensor(T).reshape(-1, 1)\n",
    "            y = torch.Tensor(y).reshape(-1, 1)\n",
    "            train_dataset = TensorDataset(X, T, y)\n",
    "            self.train_dataloader = DataLoader(\n",
    "                train_dataset, batch_size=self.batch_size, num_workers=self.num_workers\n",
    "            )\n",
    "\n",
    "    def fit(self, X, y, T, valid_perc=None):\n",
    "        \"\"\"\n",
    "        Function used to train the dragonnet model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.array\n",
    "            covariates\n",
    "        y: np.array\n",
    "            target variable\n",
    "        t: np.array\n",
    "            treatment\n",
    "        valid_perc: float\n",
    "            Percentage of data to allocate to validation set\n",
    "        \"\"\"\n",
    "        self.train_losses, self.valid_losses = [], []\n",
    "        self.create_dataloaders(X, y, T, valid_perc)\n",
    "        early_stopper = EarlyStopper(patience=10, min_delta=0)\n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss_train = 0.0\n",
    "            for batch, (X, tr, y1) in enumerate(self.train_dataloader):\n",
    "                # <--- QUAN TRỌNG: Move batch data lên GPU\n",
    "                X = X.to(self.model_device)\n",
    "                tr = tr.to(self.model_device)\n",
    "                y1 = y1.to(self.model_device)\n",
    "\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                y0_pred, y1_pred, t_pred, eps = self.model(X)\n",
    "                loss = self.loss_f(y1, tr, t_pred, y0_pred, y1_pred, eps)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                running_loss_train += loss.item()\n",
    "\n",
    "            train_loss = running_loss_train/len(self.train_dataloader)\n",
    "            self.train_losses.append(train_loss)\n",
    "            if self.valid_dataloader:\n",
    "                self.model.eval()\n",
    "                valid_loss = self.validate_step()\n",
    "                self.valid_losses.append(valid_loss)\n",
    "                # print(\n",
    "                #     f\"epoch: {epoch}--------- train_loss: {train_loss:.4f} ----- valid_loss: {valid_loss}\"\n",
    "                # )\n",
    "                self.model.train()\n",
    "                if early_stopper.early_stop(valid_loss):\n",
    "                    print(\"Early stopping activated\")\n",
    "                    break\n",
    "            else:\n",
    "                # print(f\"epoch: {epoch}--------- train_loss: {train_loss:.4f}\")\n",
    "                pass\n",
    "\n",
    "    def validate_step(self):\n",
    "        \"\"\"\n",
    "        Calculates validation loss\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        valid_loss: torch.Tensor\n",
    "            validation loss\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        valid_loss = []\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, tr, y1) in enumerate(self.valid_dataloader):\n",
    "                # <--- QUAN TRỌNG: Move batch data lên GPU\n",
    "                X = X.to(self.model_device)\n",
    "                tr = tr.to(self.model_device)\n",
    "                y1 = y1.to(self.model_device)\n",
    "\n",
    "                y0_pred, y1_pred, t_pred, eps = self.model(X)\n",
    "                loss = self.loss_f(y1, tr, t_pred, y0_pred, y1_pred, eps)\n",
    "                valid_loss.append(loss)\n",
    "        return torch.Tensor(valid_loss).mean()\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Function used to predict on covariates.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: torch.Tensor or numpy.array\n",
    "            covariates\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y0_pred: torch.Tensor\n",
    "            outcome under control\n",
    "        y1_pred: torch.Tensor\n",
    "            outcome under treatment\n",
    "        t_pred: torch.Tensor\n",
    "            predicted treatment\n",
    "        eps: torch.Tensor\n",
    "            trainable epsilon parameter\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        X = torch.Tensor(X).to(self.model_device) # <--- Move input lên GPU\n",
    "        with torch.no_grad():\n",
    "            y0_pred, y1_pred, t_pred, eps = self.model(X)\n",
    "        return (\n",
    "            y0_pred.cpu().numpy(),\n",
    "            y1_pred.cpu().numpy(),\n",
    "            t_pred.cpu().numpy(),\n",
    "            eps.cpu().numpy()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8fe0a",
   "metadata": {
    "papermill": {
     "duration": 0.004092,
     "end_time": "2025-12-22T16:38:29.982700",
     "exception": false,
     "start_time": "2025-12-22T16:38:29.978608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ace82a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T16:38:29.992084Z",
     "iopub.status.busy": "2025-12-22T16:38:29.991287Z",
     "iopub.status.idle": "2025-12-22T17:13:04.930755Z",
     "shell.execute_reply": "2025-12-22T17:13:04.929621Z"
    },
    "papermill": {
     "duration": 2074.946406,
     "end_time": "2025-12-22T17:13:04.932948",
     "exception": false,
     "start_time": "2025-12-22T16:38:29.986542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training DragonNet for Control vs Treatment 1 ===\n",
      "Early stopping activated\n",
      "MSE:  0.024297721258878587\n",
      "Uplift AUC:  0.014250053336391014\n",
      "\n",
      "=== Training DragonNet for Control vs Treatment 2 ===\n",
      "Early stopping activated\n",
      "MSE:  0.026008259443829413\n",
      "Uplift AUC:  0.002905677829647605\n",
      "\n",
      "=== Training DragonNet for Control vs Treatment 3 ===\n",
      "Early stopping activated\n",
      "MSE:  0.024332807142681167\n",
      "Uplift AUC:  -0.004758107745192709\n",
      "\n",
      "=== Training DragonNet for Control vs Treatment 4 ===\n",
      "Early stopping activated\n",
      "MSE:  0.02542913326268863\n",
      "Uplift AUC:  0.004621955306152028\n",
      "\n",
      "=== Training DragonNet for Control vs Treatment 5 ===\n",
      "Early stopping activated\n",
      "MSE:  0.02557554413435204\n",
      "Uplift AUC:  -0.007403355088457807\n",
      "\n",
      "=== Training DragonNet for Control vs Treatment 6 ===\n",
      "Early stopping activated\n",
      "MSE:  0.024991057266101034\n",
      "Uplift AUC:  0.003961910657454211\n",
      "\n",
      "All models trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklift.metrics import uplift_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "models = {} # Store one model per treatment\n",
    "\n",
    "treatment_list = [1, 2, 3, 4, 5, 6]\n",
    "models = {} # Store one model per treatment\n",
    "\n",
    "for treat_id in treatment_list:\n",
    "    print(f\"\\n=== Training DragonNet for Control vs Treatment {treat_id} ===\")\n",
    "    \n",
    "    # 1. Filter Data: Keep only Control (0) and Current Treatment (treat_id)\n",
    "    mask = np.isin(T, [0, treat_id])\n",
    "    X_sub = X[mask]\n",
    "    Y_sub = Y[mask]\n",
    "    T_sub = T[mask]\n",
    "\n",
    "    # 2. Binarize Treatment: Map treat_id -> 1, Control -> 0\n",
    "    T_sub_binary = np.where(T_sub == treat_id, 1, 0)\n",
    "    \n",
    "    # Split train test\n",
    "    X_train, X_test, Y_train, Y_test, T_train, T_test = train_test_split(\n",
    "        X_sub, Y_sub, T_sub_binary, test_size=0.2, random_state=42, stratify=T_sub\n",
    "    )\n",
    "    \n",
    "    # 2. Binarize Treatment: Map treat_id -> 1, Control -> 0\n",
    "    # This is crucial for your model's binary propensity head\n",
    "    T_sub_binary = np.where(T_sub == treat_id, 1, 0)\n",
    "    \n",
    "    # 3. Initialize Model\n",
    "    dragonnet_model = Dragonnet(\n",
    "        input_dim=X_sub.shape[1],\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # 4. Train\n",
    "    dragonnet_model.fit(X_train, Y_train, T_train, valid_perc=0.2)\n",
    "    \n",
    "    # 5. Predict and Evaluate\n",
    "    y0_pred, y1_pred, t_pred, eps = dragonnet_model.predict(X_test)\n",
    "    y_pred_final = T_test.flatten() * y1_pred.flatten() + (1 - T_test.flatten()) * y0_pred.flatten()\n",
    "    \n",
    "    mse = mean_squared_error(Y_test, y_pred_final)\n",
    "    uplift = y1_pred - y0_pred\n",
    "    auc = uplift_auc_score(Y_test, uplift, T_test)\n",
    "    \n",
    "    print(\"MSE: \", mse)\n",
    "    print(\"Uplift AUC: \", auc)\n",
    "    \n",
    "    # 6. Store model\n",
    "    models[f\"treatment_{treat_id}\"] = dragonnet_model\n",
    "\n",
    "print(\"\\nAll models trained.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2115.301838,
   "end_time": "2025-12-22T17:13:08.091224",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-22T16:37:52.789386",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
